# Feature Implementation Guide

This guide provides detailed implementation plans for 10 beginner-friendly features that can be added to nanochat. Each feature includes detailed explanations, code snippets, and step-by-step implementation instructions.

**Implementation Status:** 2/10 features completed âœ…
- Feature 1: Interactive Tokenizer Playground âœ… **IMPLEMENTED**
- Feature 5: Model Size & Cost Calculator âœ… **IMPLEMENTED**

## Table of Contents
1. [Feature 1: Interactive Tokenizer Playground](#feature-1-interactive-tokenizer-playground) âœ… **IMPLEMENTED**
2. [Feature 2: Training Progress Dashboard](#feature-2-training-progress-dashboard)
3. [Feature 3: Checkpoint Browser & Comparator](#feature-3-checkpoint-browser--comparator)
4. [Feature 4: Dataset Inspector](#feature-4-dataset-inspector)
5. [Feature 5: Model Size & Cost Calculator](#feature-5-model-size--cost-calculator) âœ… **IMPLEMENTED**
6. [Feature 6: Generation Parameter Explorer](#feature-6-generation-parameter-explorer)
7. [Feature 7: Training Resume Helper](#feature-7-training-resume-helper)
8. [Feature 8: Simple Attention Visualizer](#feature-8-simple-attention-visualizer)
9. [Feature 9: Learning Rate Finder](#feature-9-learning-rate-finder)
10. [Feature 10: Conversation Template Builder](#feature-10-conversation-template-builder)

---

## Feature 1: Interactive Tokenizer Playground

âœ… **STATUS: IMPLEMENTED** - Available in `tools/tokenizer_playground.py`

### Why This Feature is Useful

**Problem it solves:**
- Beginners struggle to understand tokenization - how "Hello world" becomes `[15496, 995]`
- Hard to debug tokenization issues (wrong special tokens, encoding errors)
- Can't visualize BPE merges and token boundaries
- No way to compare different tokenizers

**Learning benefits:**
- **Concrete understanding**: See tokenization happen in real-time
- **Debug tool**: Verify your tokenizer is working correctly
- **Comparison**: Understand differences between GPT-2 and custom tokenizers
- **Token efficiency**: See how many tokens different phrasings use

**Practical benefits:**
- Estimate token costs before training
- Optimize prompts to use fewer tokens
- Understand why model fails on certain inputs
- Validate custom tokenizer training

### Implementation Details

#### New Files to Create

**File 1: `tools/tokenizer_playground.py`**

```python
"""
Interactive Tokenizer Playground

Visualize how text is tokenized, compare tokenizers, and understand BPE.
"""

import argparse
from typing import List, Tuple
from nanochat.tokenizer import get_tokenizer, RustBPETokenizer

class TokenizerPlayground:
    """Interactive tool for exploring tokenization."""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def tokenize_and_visualize(self, text: str) -> None:
        """
        Tokenize text and display results with colors and details.

        Args:
            text: Input text to tokenize
        """
        # Encode text
        token_ids = self.tokenizer.encode(text)

        # Color codes for terminal output
        COLORS = [
            '\033[91m',  # Red
            '\033[92m',  # Green
            '\033[93m',  # Yellow
            '\033[94m',  # Blue
            '\033[95m',  # Magenta
            '\033[96m',  # Cyan
        ]
        RESET = '\033[0m'
        BOLD = '\033[1m'

        print(f"\n{BOLD}Original Text:{RESET}")
        print(f'"{text}"')
        print(f"\n{BOLD}Tokenization Results:{RESET}")
        print(f"Total tokens: {len(token_ids)}")
        print(f"Total bytes: {len(text.encode('utf-8'))}")
        print(f"Compression ratio: {len(token_ids) / len(text.encode('utf-8')):.2f} tokens/byte")

        print(f"\n{BOLD}Tokens (colored by position):{RESET}")
        for i, token_id in enumerate(token_ids):
            token_str = self.tokenizer.decode([token_id])
            color = COLORS[i % len(COLORS)]

            # Escape special characters for display
            display_str = repr(token_str)[1:-1]  # Remove quotes

            print(f"{color}[{token_id:5d}] {display_str:20s}{RESET}", end="")
            if (i + 1) % 3 == 0:  # 3 tokens per line
                print()
        print()

        print(f"\n{BOLD}Detailed Token Information:{RESET}")
        print(f"{'Index':<6} {'Token ID':<10} {'Text':<30} {'Bytes':<8} {'Type':<15}")
        print("-" * 80)

        for i, token_id in enumerate(token_ids):
            token_str = self.tokenizer.decode([token_id])
            token_bytes = token_str.encode('utf-8')
            num_bytes = len(token_bytes)

            # Determine token type
            if token_id < 256:
                token_type = "Single byte"
            elif token_str in self.tokenizer.get_special_tokens():
                token_type = "Special token"
            elif token_str.isspace():
                token_type = "Whitespace"
            elif token_str.isalpha():
                token_type = "Alphabetic"
            elif token_str.isdigit():
                token_type = "Numeric"
            else:
                token_type = "Mixed/Other"

            # Truncate display
            display_str = repr(token_str)[1:-1]
            if len(display_str) > 28:
                display_str = display_str[:25] + "..."

            print(f"{i:<6} {token_id:<10} {display_str:<30} {num_bytes:<8} {token_type:<15}")

    def compare_tokenizers(self, text: str, tokenizer2_name: str = "gpt2") -> None:
        """
        Compare current tokenizer with another (e.g., GPT-2).

        Args:
            text: Text to tokenize
            tokenizer2_name: Name of tokenizer to compare with
        """
        # Tokenize with current tokenizer
        tokens1 = self.tokenizer.encode(text)

        # Load comparison tokenizer
        tokenizer2 = RustBPETokenizer.from_pretrained(tokenizer2_name)
        tokens2 = tokenizer2.encode(text)

        print(f"\n{'='*80}")
        print(f"TOKENIZER COMPARISON")
        print(f"{'='*80}\n")

        print(f"Text: \"{text}\"\n")

        print(f"{'Tokenizer':<20} {'# Tokens':<15} {'Compression':<15}")
        print("-" * 50)
        print(f"{'Custom (yours)':<20} {len(tokens1):<15} {len(tokens1)/len(text.encode('utf-8')):.3f} tok/byte")
        print(f"{tokenizer2_name:<20} {len(tokens2):<15} {len(tokens2)/len(text.encode('utf-8')):.3f} tok/byte")

        print(f"\n{'='*80}")
        print(f"CUSTOM TOKENIZER TOKENS:")
        print(f"{'='*80}")
        for i, tid in enumerate(tokens1):
            print(f"[{tid}] {repr(self.tokenizer.decode([tid]))}", end="  ")
            if (i + 1) % 5 == 0:
                print()
        print()

        print(f"\n{'='*80}")
        print(f"{tokenizer2_name.upper()} TOKENIZER TOKENS:")
        print(f"{'='*80}")
        for i, tid in enumerate(tokens2):
            print(f"[{tid}] {repr(tokenizer2.decode([tid]))}", end="  ")
            if (i + 1) % 5 == 0:
                print()
        print()

    def analyze_special_tokens(self) -> None:
        """Display all special tokens and their IDs."""
        special_tokens = self.tokenizer.get_special_tokens()

        print(f"\n{'='*80}")
        print(f"SPECIAL TOKENS")
        print(f"{'='*80}\n")

        print(f"Total special tokens: {len(special_tokens)}\n")

        print(f"{'Token':<30} {'ID':<10}")
        print("-" * 40)

        for token in special_tokens:
            token_id = self.tokenizer.encode_special(token)
            print(f"{token:<30} {token_id:<10}")

    def interactive_mode(self) -> None:
        """Run interactive mode where user can input text repeatedly."""
        print("\n" + "="*80)
        print("TOKENIZER PLAYGROUND - Interactive Mode")
        print("="*80)
        print("\nCommands:")
        print("  - Type text to tokenize it")
        print("  - 'compare <text>' to compare with GPT-2")
        print("  - 'special' to show special tokens")
        print("  - 'quit' to exit")
        print()

        while True:
            try:
                user_input = input("\n> ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("Goodbye!")
                    break

                if user_input.lower() == 'special':
                    self.analyze_special_tokens()
                    continue

                if user_input.lower().startswith('compare '):
                    text = user_input[8:].strip()
                    if text:
                        self.compare_tokenizers(text)
                    else:
                        print("Usage: compare <text>")
                    continue

                # Default: tokenize the input
                self.tokenize_and_visualize(user_input)

            except KeyboardInterrupt:
                print("\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {e}")


def main():
    parser = argparse.ArgumentParser(description="Interactive Tokenizer Playground")
    parser.add_argument("text", nargs="?", help="Text to tokenize (optional, use interactive mode if not provided)")
    parser.add_argument("--compare", action="store_true", help="Compare with GPT-2 tokenizer")
    parser.add_argument("--special", action="store_true", help="Show special tokens")
    parser.add_argument("--interactive", "-i", action="store_true", help="Run in interactive mode")

    args = parser.parse_args()

    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = get_tokenizer()
    playground = TokenizerPlayground(tokenizer)

    # Run appropriate mode
    if args.special:
        playground.analyze_special_tokens()
    elif args.interactive or not args.text:
        playground.interactive_mode()
    elif args.compare:
        playground.compare_tokenizers(args.text)
    else:
        playground.tokenize_and_visualize(args.text)


if __name__ == "__main__":
    main()
```

#### Files to Modify

**No existing files need to be modified** - this is a standalone tool.

#### Usage Examples

```bash
# Interactive mode
python tools/tokenizer_playground.py

# Tokenize a specific string
python tools/tokenizer_playground.py "Hello, world!"

# Compare with GPT-2
python tools/tokenizer_playground.py "Hello, world!" --compare

# Show special tokens
python tools/tokenizer_playground.py --special
```

#### Expected Output

```
Original Text:
"Hello, world!"

Tokenization Results:
Total tokens: 4
Total bytes: 13
Compression ratio: 0.31 tokens/byte

Tokens (colored by position):
[15496] Hello               [   11] ,                   [  995] world
[    0] !

Detailed Token Information:
Index  Token ID   Text                           Bytes    Type
--------------------------------------------------------------------------------
0      15496      Hello                          5        Alphabetic
1      11         ,                              1        Mixed/Other
2      995        world                          5        Alphabetic
3      0          !                              1        Mixed/Other
```

### Learning Outcomes

After implementing this feature, you'll understand:
- âœ… How to decode token IDs back to text
- âœ… How to use ANSI color codes for terminal output
- âœ… String formatting and text manipulation in Python
- âœ… Interactive command-line interfaces
- âœ… The relationship between bytes and tokens

### Testing Approach

```python
# Test cases to verify
test_cases = [
    "Hello, world!",
    "The quick brown fox",
    "What is 2+2?",
    "<|user_start|>Hello<|user_end|>",  # Test special tokens
    "ðŸ˜€ðŸŽ‰",  # Test unicode/emoji
    "   spaces   ",  # Test whitespace
    "123456789",  # Test numbers
]

for text in test_cases:
    playground.tokenize_and_visualize(text)
```

---

## Feature 2: Training Progress Dashboard

### Why This Feature is Useful

**Problem it solves:**
- Training runs for hours with minimal feedback
- Can't see if training is progressing well
- No visual indication of loss decreasing
- Hard to spot problems early (divergence, overfitting)
- Don't know when training will finish

**Learning benefits:**
- **Visualization**: Understand training dynamics by seeing curves
- **Pattern recognition**: Learn what healthy vs unhealthy training looks like
- **Hyperparameter effects**: See how LR, batch size affect training

**Practical benefits:**
- Catch problems early (stop bad runs)
- Estimate time remaining
- Monitor GPU utilization
- Save best checkpoints automatically
- Generate progress reports to share

### Implementation Details

#### New Files to Create

**File 1: `nanochat/training_dashboard.py`**

```python
"""
Training Progress Dashboard

Real-time visualization of training metrics with plots and statistics.
"""

import os
import time
import json
from collections import deque
from typing import Dict, List, Optional
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
from datetime import datetime, timedelta


class TrainingDashboard:
    """
    Tracks and visualizes training metrics in real-time.

    Saves plots to disk and maintains a rolling window of recent metrics.
    """

    def __init__(self, output_dir: str, window_size: int = 1000):
        """
        Args:
            output_dir: Directory to save plots and logs
            window_size: Number of recent steps to keep in memory
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        self.window_size = window_size

        # Metric history (deques for efficient append/popleft)
        self.steps = deque(maxlen=window_size)
        self.train_losses = deque(maxlen=window_size)
        self.val_losses = deque(maxlen=window_size)
        self.learning_rates = deque(maxlen=window_size)
        self.tokens_per_sec = deque(maxlen=window_size)
        self.gpu_memory_mb = deque(maxlen=window_size)

        # Timing
        self.start_time = time.time()
        self.last_step_time = time.time()

        # Best model tracking
        self.best_val_loss = float('inf')
        self.best_step = 0

    def log_step(self,
                  step: int,
                  train_loss: Optional[float] = None,
                  val_loss: Optional[float] = None,
                  lr: Optional[float] = None,
                  tokens_per_sec: Optional[float] = None,
                  gpu_memory_mb: Optional[float] = None):
        """
        Log metrics for a training step.

        Args:
            step: Current training step
            train_loss: Training loss (if computed)
            val_loss: Validation loss (if computed)
            lr: Current learning rate
            tokens_per_sec: Training throughput
            gpu_memory_mb: GPU memory usage in MB
        """
        self.steps.append(step)

        if train_loss is not None:
            self.train_losses.append(train_loss)
        if val_loss is not None:
            self.val_losses.append(val_loss)
            # Track best validation loss
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.best_step = step
        if lr is not None:
            self.learning_rates.append(lr)
        if tokens_per_sec is not None:
            self.tokens_per_sec.append(tokens_per_sec)
        if gpu_memory_mb is not None:
            self.gpu_memory_mb.append(gpu_memory_mb)

    def estimate_time_remaining(self, current_step: int, total_steps: int) -> str:
        """
        Estimate time remaining based on average step time.

        Returns:
            Human-readable time estimate (e.g., "2h 15m")
        """
        if current_step == 0:
            return "Unknown"

        elapsed = time.time() - self.start_time
        avg_step_time = elapsed / current_step
        remaining_steps = total_steps - current_step
        remaining_seconds = avg_step_time * remaining_steps

        # Format as hours and minutes
        hours = int(remaining_seconds // 3600)
        minutes = int((remaining_seconds % 3600) // 60)

        if hours > 0:
            return f"{hours}h {minutes}m"
        else:
            return f"{minutes}m"

    def plot_metrics(self, save_path: Optional[str] = None):
        """
        Create a multi-panel plot of all metrics.

        Args:
            save_path: Path to save the plot (default: output_dir/training_progress.png)
        """
        if save_path is None:
            save_path = os.path.join(self.output_dir, "training_progress.png")

        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Progress Dashboard', fontsize=16, fontweight='bold')

        steps_list = list(self.steps)

        # Plot 1: Loss curves
        ax = axes[0, 0]
        if self.train_losses:
            ax.plot(steps_list[:len(self.train_losses)], list(self.train_losses),
                   label='Train Loss', color='blue', alpha=0.7)
        if self.val_losses:
            # Val losses are sparse, plot with markers
            val_steps = [s for s in steps_list if s in steps_list[:len(self.val_losses)]]
            ax.plot(val_steps, list(self.val_losses),
                   label='Val Loss', color='red', marker='o', linestyle='--')
        ax.set_xlabel('Step')
        ax.set_ylabel('Loss')
        ax.set_title('Training and Validation Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Plot 2: Learning rate schedule
        ax = axes[0, 1]
        if self.learning_rates:
            ax.plot(steps_list[:len(self.learning_rates)], list(self.learning_rates),
                   color='green')
            ax.set_xlabel('Step')
            ax.set_ylabel('Learning Rate')
            ax.set_title('Learning Rate Schedule')
            ax.grid(True, alpha=0.3)

        # Plot 3: Training throughput
        ax = axes[1, 0]
        if self.tokens_per_sec:
            ax.plot(steps_list[:len(self.tokens_per_sec)], list(self.tokens_per_sec),
                   color='orange')
            ax.set_xlabel('Step')
            ax.set_ylabel('Tokens/Second')
            ax.set_title('Training Throughput')
            ax.grid(True, alpha=0.3)

        # Plot 4: GPU memory usage
        ax = axes[1, 1]
        if self.gpu_memory_mb:
            ax.plot(steps_list[:len(self.gpu_memory_mb)], list(self.gpu_memory_mb),
                   color='purple')
            ax.set_xlabel('Step')
            ax.set_ylabel('Memory (MB)')
            ax.set_title('GPU Memory Usage')
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        print(f"Dashboard plot saved to: {save_path}")

    def print_status(self, step: int, total_steps: int):
        """
        Print a status summary to console.

        Args:
            step: Current step
            total_steps: Total number of steps
        """
        elapsed = time.time() - self.start_time
        eta = self.estimate_time_remaining(step, total_steps)
        progress_pct = 100.0 * step / total_steps if total_steps > 0 else 0

        print("\n" + "="*80)
        print(f"Training Dashboard - Step {step}/{total_steps} ({progress_pct:.1f}%)")
        print("="*80)

        # Current metrics
        if self.train_losses:
            print(f"Current train loss: {self.train_losses[-1]:.4f}")
        if self.val_losses:
            print(f"Current val loss:   {self.val_losses[-1]:.4f}")
            print(f"Best val loss:      {self.best_val_loss:.4f} (step {self.best_step})")
        if self.learning_rates:
            print(f"Learning rate:      {self.learning_rates[-1]:.6f}")
        if self.tokens_per_sec:
            print(f"Throughput:         {self.tokens_per_sec[-1]:.0f} tokens/sec")

        # Timing
        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        print(f"\nElapsed time:       {hours}h {minutes}m")
        print(f"ETA:                {eta}")

        print("="*80 + "\n")

    def save_summary(self):
        """Save a JSON summary of the training run."""
        summary = {
            "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
            "total_steps": len(self.steps),
            "best_val_loss": self.best_val_loss,
            "best_step": self.best_step,
            "final_train_loss": self.train_losses[-1] if self.train_losses else None,
            "final_val_loss": self.val_losses[-1] if self.val_losses else None,
            "avg_tokens_per_sec": sum(self.tokens_per_sec) / len(self.tokens_per_sec) if self.tokens_per_sec else None,
        }

        summary_path = os.path.join(self.output_dir, "training_summary.json")
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"Training summary saved to: {summary_path}")
```

#### Files to Modify

**File 1: `scripts/base_train.py`** (Add dashboard integration)

Add at the top with other imports:
```python
from nanochat.training_dashboard import TrainingDashboard
```

Add after optimizer initialization:
```python
# Initialize training dashboard
dashboard = TrainingDashboard(
    output_dir=os.path.join(base_dir, "training_logs", f"run_{run}"),
    window_size=1000
)
```

Inside the training loop, add logging:
```python
# After computing loss and metrics
dashboard.log_step(
    step=step,
    train_loss=train_loss.item(),
    val_loss=val_bpb if step % eval_every == 0 else None,
    lr=optimizers[0].param_groups[0]['lr'],
    tokens_per_sec=tok_per_sec,
    gpu_memory_mb=get_max_memory() / 1024**2 if device_type == "cuda" else None
)

# Print status every 100 steps
if step % 100 == 0:
    dashboard.print_status(step, num_iterations)

# Generate plots every 500 steps
if step % 500 == 0:
    dashboard.plot_metrics()
```

At the end of training:
```python
# Save final summary and plots
dashboard.plot_metrics()
dashboard.save_summary()
```

#### Usage

The dashboard runs automatically during training. After running:

```bash
python -m scripts.base_train --run=my_experiment
```

You'll see:
1. Console status updates every 100 steps
2. Plots saved to `out/training_logs/run_my_experiment/training_progress.png`
3. JSON summary at end

#### Expected Output

Console:
```
================================================================================
Training Dashboard - Step 1000/5400 (18.5%)
================================================================================
Current train loss: 2.3456
Current val loss:   2.4123
Best val loss:      2.4001 (step 750)
Learning rate:      0.019500
Throughput:         12500 tokens/sec

Elapsed time:       0h 45m
ETA:                3h 20m
================================================================================
```

Plot shows 4 panels with loss curves, LR schedule, throughput, and memory usage.

### Learning Outcomes

- âœ… Using matplotlib for visualization
- âœ… Efficient data structures (deques) for rolling windows
- âœ… Time estimation algorithms
- âœ… JSON serialization for logging
- âœ… Integration with existing training code

---

## Feature 3: Checkpoint Browser & Comparator

### Why This Feature is Useful

**Problem it solves:**
- Don't know which checkpoint is best
- Checkpoints accumulate and waste disk space
- Can't remember what each checkpoint was trained with
- Hard to compare different training runs

**Learning benefits:**
- **Checkpoint structure**: Understand what's saved in checkpoints
- **Model metadata**: Learn about configuration and hyperparameters
- **Version control**: Track experiments systematically

**Practical benefits:**
- Quickly find best checkpoint
- Clean up old checkpoints
- Compare different model sizes
- Resume from specific checkpoints
- Share models with metadata

### Implementation Details

#### New Files to Create

**File 1: `tools/checkpoint_browser.py`**

```python
"""
Checkpoint Browser and Comparator

Browse, inspect, and compare model checkpoints.
"""

import os
import argparse
from pathlib import Path
from typing import List, Dict, Optional
import torch
from datetime import datetime
from tabulate import tabulate


class CheckpointBrowser:
    """Tool for browsing and comparing model checkpoints."""

    def __init__(self, base_dir: str = "out"):
        self.base_dir = base_dir
        self.checkpoint_dirs = [
            os.path.join(base_dir, "base_checkpoints"),
            os.path.join(base_dir, "chat_checkpoints"),
        ]

    def find_all_checkpoints(self) -> List[Dict]:
        """
        Scan for all checkpoints and gather metadata.

        Returns:
            List of checkpoint info dictionaries
        """
        checkpoints = []

        for checkpoint_dir in self.checkpoint_dirs:
            if not os.path.exists(checkpoint_dir):
                continue

            for model_name in os.listdir(checkpoint_dir):
                model_path = os.path.join(checkpoint_dir, model_name)
                if not os.path.isdir(model_path):
                    continue

                checkpoint_file = os.path.join(model_path, "checkpoint.pt")
                if not os.path.exists(checkpoint_file):
                    continue

                # Load checkpoint metadata (not full model)
                try:
                    checkpoint = torch.load(checkpoint_file, map_location='cpu')

                    # Extract metadata
                    metadata = checkpoint.get('metadata', {})
                    model_config = metadata.get('model_config', {})

                    # File stats
                    stat = os.stat(checkpoint_file)
                    size_mb = stat.st_size / (1024 * 1024)
                    modified_time = datetime.fromtimestamp(stat.st_mtime)

                    # Calculate number of parameters
                    model_state = checkpoint.get('model', {})
                    num_params = sum(p.numel() for p in model_state.values())

                    checkpoint_info = {
                        'name': model_name,
                        'path': checkpoint_file,
                        'type': 'base' if 'base_checkpoints' in checkpoint_dir else 'chat',
                        'step': metadata.get('step', 'Unknown'),
                        'val_bpb': metadata.get('val_bpb', None),
                        'num_params': num_params,
                        'depth': model_config.get('n_layer', 'Unknown'),
                        'hidden_dim': model_config.get('n_embd', 'Unknown'),
                        'size_mb': size_mb,
                        'modified': modified_time,
                        'metadata': metadata,
                    }

                    checkpoints.append(checkpoint_info)

                except Exception as e:
                    print(f"Warning: Could not load {checkpoint_file}: {e}")

        return checkpoints

    def list_checkpoints(self, sort_by: str = 'modified'):
        """
        List all checkpoints in a formatted table.

        Args:
            sort_by: Field to sort by ('modified', 'val_bpb', 'size_mb', 'num_params')
        """
        checkpoints = self.find_all_checkpoints()

        if not checkpoints:
            print("No checkpoints found!")
            return

        # Sort checkpoints
        if sort_by == 'modified':
            checkpoints.sort(key=lambda x: x['modified'], reverse=True)
        elif sort_by == 'val_bpb':
            checkpoints.sort(key=lambda x: x['val_bpb'] if x['val_bpb'] else float('inf'))
        elif sort_by == 'size_mb':
            checkpoints.sort(key=lambda x: x['size_mb'], reverse=True)
        elif sort_by == 'num_params':
            checkpoints.sort(key=lambda x: x['num_params'], reverse=True)

        # Prepare table data
        headers = ['Name', 'Type', 'Step', 'Val BPB', 'Params (M)', 'Depth', 'Dim', 'Size (MB)', 'Modified']
        rows = []

        for cp in checkpoints:
            rows.append([
                cp['name'],
                cp['type'],
                cp['step'],
                f"{cp['val_bpb']:.4f}" if cp['val_bpb'] else 'N/A',
                f"{cp['num_params']/1e6:.1f}",
                cp['depth'],
                cp['hidden_dim'],
                f"{cp['size_mb']:.1f}",
                cp['modified'].strftime('%Y-%m-%d %H:%M'),
            ])

        print("\n" + "="*120)
        print(f"CHECKPOINTS (sorted by {sort_by})")
        print("="*120 + "\n")
        print(tabulate(rows, headers=headers, tablefmt='grid'))
        print(f"\nTotal checkpoints: {len(checkpoints)}")
        total_size_gb = sum(cp['size_mb'] for cp in checkpoints) / 1024
        print(f"Total disk usage: {total_size_gb:.2f} GB\n")

    def inspect_checkpoint(self, name: str):
        """
        Show detailed information about a specific checkpoint.

        Args:
            name: Checkpoint name (e.g., 'd20')
        """
        checkpoints = self.find_all_checkpoints()
        checkpoint = next((cp for cp in checkpoints if cp['name'] == name), None)

        if not checkpoint:
            print(f"Checkpoint '{name}' not found!")
            return

        print("\n" + "="*80)
        print(f"CHECKPOINT DETAILS: {name}")
        print("="*80 + "\n")

        # Basic info
        print(f"Type:           {checkpoint['type']}")
        print(f"Path:           {checkpoint['path']}")
        print(f"Modified:       {checkpoint['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Size:           {checkpoint['size_mb']:.1f} MB")

        # Training info
        print(f"\nTraining Information:")
        print(f"  Step:         {checkpoint['step']}")
        print(f"  Val BPB:      {checkpoint['val_bpb']:.4f}" if checkpoint['val_bpb'] else "  Val BPB:      N/A")

        # Model architecture
        print(f"\nModel Architecture:")
        print(f"  Parameters:   {checkpoint['num_params']/1e6:.1f}M")
        print(f"  Depth:        {checkpoint['depth']} layers")
        print(f"  Hidden dim:   {checkpoint['hidden_dim']}")

        # Full metadata
        metadata = checkpoint['metadata']
        if 'user_config' in metadata:
            print(f"\nUser Configuration:")
            for key, value in metadata['user_config'].items():
                print(f"  {key:20s} = {value}")

        print()

    def compare_checkpoints(self, name1: str, name2: str):
        """
        Compare two checkpoints side-by-side.

        Args:
            name1: First checkpoint name
            name2: Second checkpoint name
        """
        checkpoints = self.find_all_checkpoints()
        cp1 = next((cp for cp in checkpoints if cp['name'] == name1), None)
        cp2 = next((cp for cp in checkpoints if cp['name'] == name2), None)

        if not cp1:
            print(f"Checkpoint '{name1}' not found!")
            return
        if not cp2:
            print(f"Checkpoint '{name2}' not found!")
            return

        print("\n" + "="*100)
        print(f"CHECKPOINT COMPARISON")
        print("="*100 + "\n")

        # Comparison table
        headers = ['Metric', name1, name2, 'Difference']
        rows = []

        # Parameters
        diff_params = (cp2['num_params'] - cp1['num_params']) / 1e6
        rows.append([
            'Parameters (M)',
            f"{cp1['num_params']/1e6:.1f}",
            f"{cp2['num_params']/1e6:.1f}",
            f"{diff_params:+.1f}M"
        ])

        # Depth
        rows.append([
            'Depth (layers)',
            str(cp1['depth']),
            str(cp2['depth']),
            str(cp2['depth'] - cp1['depth']) if isinstance(cp1['depth'], int) and isinstance(cp2['depth'], int) else 'N/A'
        ])

        # Hidden dim
        rows.append([
            'Hidden dimension',
            str(cp1['hidden_dim']),
            str(cp2['hidden_dim']),
            str(cp2['hidden_dim'] - cp1['hidden_dim']) if isinstance(cp1['hidden_dim'], int) and isinstance(cp2['hidden_dim'], int) else 'N/A'
        ])

        # Val BPB
        if cp1['val_bpb'] and cp2['val_bpb']:
            diff_bpb = cp2['val_bpb'] - cp1['val_bpb']
            rows.append([
                'Val BPB',
                f"{cp1['val_bpb']:.4f}",
                f"{cp2['val_bpb']:.4f}",
                f"{diff_bpb:+.4f}"
            ])

        # Size
        diff_size = cp2['size_mb'] - cp1['size_mb']
        rows.append([
            'Checkpoint size (MB)',
            f"{cp1['size_mb']:.1f}",
            f"{cp2['size_mb']:.1f}",
            f"{diff_size:+.1f}"
        ])

        print(tabulate(rows, headers=headers, tablefmt='grid'))
        print()

    def delete_checkpoint(self, name: str, confirm: bool = False):
        """
        Delete a checkpoint (with confirmation).

        Args:
            name: Checkpoint name to delete
            confirm: If True, skip confirmation prompt
        """
        checkpoints = self.find_all_checkpoints()
        checkpoint = next((cp for cp in checkpoints if cp['name'] == name), None)

        if not checkpoint:
            print(f"Checkpoint '{name}' not found!")
            return

        print(f"\nCheckpoint to delete:")
        print(f"  Name: {checkpoint['name']}")
        print(f"  Path: {checkpoint['path']}")
        print(f"  Size: {checkpoint['size_mb']:.1f} MB")

        if not confirm:
            response = input("\nAre you sure you want to delete this checkpoint? (yes/no): ")
            if response.lower() != 'yes':
                print("Deletion cancelled.")
                return

        # Delete the checkpoint directory
        checkpoint_dir = os.path.dirname(checkpoint['path'])
        import shutil
        shutil.rmtree(checkpoint_dir)

        print(f"âœ“ Deleted checkpoint: {name}")


def main():
    parser = argparse.ArgumentParser(description="Checkpoint Browser and Comparator")
    parser.add_argument("--list", action="store_true", help="List all checkpoints")
    parser.add_argument("--sort", default="modified", choices=['modified', 'val_bpb', 'size_mb', 'num_params'],
                       help="Sort checkpoints by field")
    parser.add_argument("--inspect", metavar="NAME", help="Inspect a specific checkpoint")
    parser.add_argument("--compare", nargs=2, metavar=("NAME1", "NAME2"), help="Compare two checkpoints")
    parser.add_argument("--delete", metavar="NAME", help="Delete a checkpoint")
    parser.add_argument("--yes", action="store_true", help="Skip confirmation prompts")
    parser.add_argument("--base-dir", default="out", help="Base directory for checkpoints")

    args = parser.parse_args()

    browser = CheckpointBrowser(base_dir=args.base_dir)

    if args.list:
        browser.list_checkpoints(sort_by=args.sort)
    elif args.inspect:
        browser.inspect_checkpoint(args.inspect)
    elif args.compare:
        browser.compare_checkpoints(args.compare[0], args.compare[1])
    elif args.delete:
        browser.delete_checkpoint(args.delete, confirm=args.yes)
    else:
        # Default: list checkpoints
        browser.list_checkpoints()


if __name__ == "__main__":
    main()
```

#### Files to Modify

**None** - this is a standalone tool.

#### Usage Examples

```bash
# List all checkpoints
python tools/checkpoint_browser.py --list

# Sort by validation loss
python tools/checkpoint_browser.py --list --sort val_bpb

# Inspect a specific checkpoint
python tools/checkpoint_browser.py --inspect d20

# Compare two checkpoints
python tools/checkpoint_browser.py --compare d20 d26

# Delete old checkpoint
python tools/checkpoint_browser.py --delete old_experiment_d20
```

#### Expected Output

```
==============================================================================
CHECKPOINTS (sorted by modified)
==============================================================================

+----------+------+------+---------+-----------+-------+-----+----------+------------------+
| Name     | Type | Step | Val BPB | Params (M)| Depth | Dim | Size (MB)| Modified         |
+==========+======+======+=========+===========+=======+=====+==========+==================+
| d20      | base | 5400 | 1.2145  | 561.2     | 20    | 512 | 2145.3   | 2025-11-08 14:32 |
| d26      | base | 7200 | 1.1823  | 1043.5    | 26    | 832 | 3982.1   | 2025-11-07 09:15 |
| d20_sft  | chat | 2000 | 1.3421  | 561.2     | 20    | 512 | 2145.3   | 2025-11-06 18:45 |
+----------+------+------+---------+-----------+-------+-----+----------+------------------+

Total checkpoints: 3
Total disk usage: 8.27 GB
```

### Learning Outcomes

- âœ… Working with file system and pathlib
- âœ… Loading PyTorch checkpoints without full model
- âœ… Using tabulate for formatted tables
- âœ… Command-line argument parsing
- âœ… File metadata and timestamps

---

## Feature 4: Dataset Inspector

### Why This Feature is Useful

**Problem it solves:**
- Don't know what data the model is training on
- Can't verify data quality before spending hours training
- No way to check for formatting errors in conversations
- Hard to understand token distribution in dataset

**Learning benefits:**
- **Data awareness**: Understand what patterns the model learns
- **Quality control**: Catch bad data before training
- **Format verification**: Ensure conversations are structured correctly
- **Distribution analysis**: See if data is balanced

**Practical benefits:**
- Preview dataset samples
- Find and fix formatting issues
- Estimate optimal sequence length
- Balance dataset across different task types
- Export samples for manual review

### Implementation Details

#### New Files to Create

**File 1: `tools/dataset_inspector.py`**

```python
"""
Dataset Inspector

Inspect, analyze, and validate training datasets.
"""

import argparse
import json
import random
from collections import Counter, defaultdict
from typing import List, Dict
from nanochat.tokenizer import get_tokenizer


class DatasetInspector:
    """Tool for inspecting and analyzing datasets."""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def load_jsonl(self, filepath: str, max_samples: int = None) -> List[Dict]:
        """
        Load conversations from JSONL file.

        Args:
            filepath: Path to JSONL file
            max_samples: Maximum number of samples to load

        Returns:
            List of conversation dictionaries
        """
        conversations = []
        with open(filepath, 'r') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                conversations.append(json.loads(line))
        return conversations

    def show_samples(self, filepath: str, num_samples: int = 5):
        """
        Show random samples from dataset.

        Args:
            filepath: Path to dataset file
            num_samples: Number of samples to show
        """
        conversations = self.load_jsonl(filepath)
        samples = random.sample(conversations, min(num_samples, len(conversations)))

        print(f"\n{'='*80}")
        print(f"DATASET SAMPLES ({num_samples} of {len(conversations)} total)")
        print(f"{'='*80}\n")

        for i, conv in enumerate(samples, 1):
            print(f"Sample {i}:")
            print("-" * 80)

            for msg in conv['messages']:
                role = msg['role'].upper()
                content = msg['content']

                # Handle structured content (with tool use)
                if isinstance(content, str):
                    print(f"{role}: {content}")
                elif isinstance(content, list):
                    print(f"{role}:")
                    for part in content:
                        if part['type'] == 'text':
                            print(f"  TEXT: {part['text']}")
                        elif part['type'] == 'python':
                            print(f"  CODE: {part['text']}")
                        elif part['type'] == 'python_output':
                            print(f"  OUTPUT: {part['text']}")

            print()

    def analyze_lengths(self, filepath: str):
        """
        Analyze token lengths in dataset.

        Args:
            filepath: Path to dataset file
        """
        conversations = self.load_jsonl(filepath)

        token_lengths = []
        user_lengths = []
        assistant_lengths = []

        for conv in conversations:
            # Tokenize full conversation
            ids, _ = self.tokenizer.render_conversation(conv)
            token_lengths.append(len(ids))

            # Analyze individual messages
            for msg in conv['messages']:
                if msg['role'] == 'user':
                    if isinstance(msg['content'], str):
                        user_lengths.append(len(self.tokenizer.encode(msg['content'])))
                elif msg['role'] == 'assistant':
                    if isinstance(msg['content'], str):
                        asst_tokens = self.tokenizer.encode(msg['content'])
                        assistant_lengths.append(len(asst_tokens))

        print(f"\n{'='*80}")
        print(f"LENGTH ANALYSIS")
        print(f"{'='*80}\n")

        print(f"Total conversations: {len(conversations)}")
        print(f"\nFull Conversation Lengths (in tokens):")
        print(f"  Min:     {min(token_lengths)}")
        print(f"  Max:     {max(token_lengths)}")
        print(f"  Mean:    {sum(token_lengths)/len(token_lengths):.1f}")
        print(f"  Median:  {sorted(token_lengths)[len(token_lengths)//2]}")

        print(f"\nUser Message Lengths:")
        if user_lengths:
            print(f"  Min:     {min(user_lengths)}")
            print(f"  Max:     {max(user_lengths)}")
            print(f"  Mean:    {sum(user_lengths)/len(user_lengths):.1f}")

        print(f"\nAssistant Message Lengths:")
        if assistant_lengths:
            print(f"  Min:     {min(assistant_lengths)}")
            print(f"  Max:     {max(assistant_lengths)}")
            print(f"  Mean:    {sum(assistant_lengths)/len(assistant_lengths):.1f}")

        # Show distribution histogram
        print(f"\nLength Distribution (# conversations):")
        bins = [0, 100, 200, 500, 1000, 2000, 5000]
        distribution = defaultdict(int)

        for length in token_lengths:
            for i, bin_max in enumerate(bins[1:], 1):
                if length <= bin_max:
                    bin_label = f"{bins[i-1]}-{bin_max}"
                    distribution[bin_label] += 1
                    break
            else:
                distribution[f">{bins[-1]}"] += 1

        for bin_label, count in sorted(distribution.items()):
            bar = '#' * (count * 50 // len(conversations))
            print(f"  {bin_label:12s}: {count:5d} {bar}")

    def validate_format(self, filepath: str):
        """
        Validate dataset format and find issues.

        Args:
            filepath: Path to dataset file
        """
        conversations = self.load_jsonl(filepath)

        print(f"\n{'='*80}")
        print(f"FORMAT VALIDATION")
        print(f"{'='*80}\n")

        issues = []
        issue_types = Counter()

        for i, conv in enumerate(conversations):
            # Check required fields
            if 'messages' not in conv:
                issues.append((i, "Missing 'messages' field"))
                issue_types['missing_messages'] += 1
                continue

            messages = conv['messages']

            # Check message count
            if len(messages) < 2:
                issues.append((i, "Less than 2 messages"))
                issue_types['too_short'] += 1

            # Check alternating roles
            for j, msg in enumerate(messages):
                expected_role = 'user' if j % 2 == 0 else 'assistant'
                if msg.get('role') != expected_role:
                    issues.append((i, f"Message {j}: expected {expected_role}, got {msg.get('role')}"))
                    issue_types['wrong_role'] += 1

                # Check content exists
                if 'content' not in msg or not msg['content']:
                    issues.append((i, f"Message {j}: empty content"))
                    issue_types['empty_content'] += 1

        # Print summary
        if issues:
            print(f"Found {len(issues)} issues:\n")
            for issue_type, count in issue_types.most_common():
                print(f"  {issue_type:20s}: {count:5d}")

            print(f"\nFirst 10 issues:")
            for idx, issue_msg in issues[:10]:
                print(f"  Conversation {idx}: {issue_msg}")
        else:
            print("âœ“ No format issues found!")

        # Additional statistics
        print(f"\n{'='*80}")
        print(f"DATASET STATISTICS")
        print(f"{'='*80}\n")

        total_messages = sum(len(conv['messages']) for conv in conversations)
        print(f"Total conversations: {len(conversations)}")
        print(f"Total messages:      {total_messages}")
        print(f"Avg messages/conv:   {total_messages/len(conversations):.1f}")

        # Count tool usage
        tool_usage = Counter()
        for conv in conversations:
            for msg in conv['messages']:
                if isinstance(msg.get('content'), list):
                    for part in msg['content']:
                        if part.get('type') in ['python', 'python_output']:
                            tool_usage['uses_tools'] += 1
                            break

        if tool_usage:
            print(f"\nTool Usage:")
            print(f"  Conversations with tools: {tool_usage['uses_tools']} ({100*tool_usage['uses_tools']/len(conversations):.1f}%)")

    def export_samples(self, filepath: str, output_file: str, num_samples: int = 100):
        """
        Export random samples to a file for manual review.

        Args:
            filepath: Input dataset path
            output_file: Output file path
            num_samples: Number of samples to export
        """
        conversations = self.load_jsonl(filepath)
        samples = random.sample(conversations, min(num_samples, len(conversations)))

        with open(output_file, 'w') as f:
            for i, conv in enumerate(samples, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"SAMPLE {i}\n")
                f.write(f"{'='*80}\n\n")

                for msg in conv['messages']:
                    role = msg['role'].upper()
                    content = msg['content']

                    if isinstance(content, str):
                        f.write(f"{role}: {content}\n\n")
                    elif isinstance(content, list):
                        f.write(f"{role}:\n")
                        for part in content:
                            f.write(f"  [{part['type'].upper()}] {part['text']}\n")
                        f.write("\n")

        print(f"âœ“ Exported {len(samples)} samples to {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Dataset Inspector")
    parser.add_argument("dataset", help="Path to dataset file (JSONL)")
    parser.add_argument("--samples", type=int, default=5, help="Number of samples to show")
    parser.add_argument("--analyze-lengths", action="store_true", help="Analyze token lengths")
    parser.add_argument("--validate", action="store_true", help="Validate format")
    parser.add_argument("--export", metavar="FILE", help="Export samples to file")
    parser.add_argument("--export-count", type=int, default=100, help="Number of samples to export")

    args = parser.parse_args()

    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = get_tokenizer()
    inspector = DatasetInspector(tokenizer)

    # Run requested operations
    if args.validate:
        inspector.validate_format(args.dataset)
    elif args.analyze_lengths:
        inspector.analyze_lengths(args.dataset)
    elif args.export:
        inspector.export_samples(args.dataset, args.export, args.export_count)
    else:
        inspector.show_samples(args.dataset, args.samples)


if __name__ == "__main__":
    main()
```

#### Files to Modify

**None** - standalone tool.

#### Usage Examples

```bash
# Show random samples
python tools/dataset_inspector.py data/train.jsonl --samples 5

# Analyze token lengths
python tools/dataset_inspector.py data/train.jsonl --analyze-lengths

# Validate format
python tools/dataset_inspector.py data/train.jsonl --validate

# Export samples for manual review
python tools/dataset_inspector.py data/train.jsonl --export review.txt --export-count 50
```

### Learning Outcomes

- âœ… Working with JSONL data format
- âœ… Statistical analysis (min, max, median, distributions)
- âœ… Data validation and error checking
- âœ… Using Counter and defaultdict from collections
- âœ… File I/O for different formats

---

## Feature 5: Model Size & Cost Calculator

âœ… **STATUS: IMPLEMENTED** - Available in `tools/model_calculator.py`

### Why This Feature is Useful

**Problem it solves:**
- Don't know if model will fit in GPU memory
- No idea how long training will take
- Can't estimate costs before starting
- Hard to choose optimal model size

**Learning benefits:**
- **Parameter counting**: Understand where parameters come from
- **Computational complexity**: Learn about FLOPs and memory
- **Resource planning**: Make informed decisions about model size

**Practical benefits:**
- Avoid OOM errors
- Budget GPU costs
- Choose appropriate batch size
- Plan experiment timelines
- Compare different architectures

### Implementation Details

#### New Files to Create

**File 1: `tools/model_calculator.py`**

```python
"""
Model Size and Cost Calculator

Calculate parameters, memory, FLOPs, and training costs for models.
"""

import argparse
import math
from typing import Dict


class ModelCalculator:
    """Calculator for model size, memory, and cost estimates."""

    # GPU specs (memory in GB, cost in $/hour)
    GPU_SPECS = {
        'H100': {'memory_gb': 80, 'cost_per_hour': 3.5, 'tflops': 1000},
        'A100': {'memory_gb': 80, 'cost_per_hour': 2.5, 'tflops': 312},
        'A100-40GB': {'memory_gb': 40, 'cost_per_hour': 2.0, 'tflops': 312},
        'V100': {'memory_gb': 32, 'cost_per_hour': 1.5, 'tflops': 125},
        'RTX4090': {'memory_gb': 24, 'cost_per_hour': 0.5, 'tflops': 330},
        'T4': {'memory_gb': 16, 'cost_per_hour': 0.35, 'tflops': 65},
    }

    def calculate_parameters(self,
                           depth: int,
                           hidden_dim: int,
                           vocab_size: int = 65536,
                           n_heads: int = None) -> Dict:
        """
        Calculate number of parameters in model.

        Args:
            depth: Number of transformer layers
            hidden_dim: Hidden dimension
            vocab_size: Vocabulary size
            n_heads: Number of attention heads (auto-calculated if None)

        Returns:
            Dictionary with parameter breakdown
        """
        if n_heads is None:
            n_heads = max(1, (hidden_dim + 127) // 128)

        head_dim = hidden_dim // n_heads

        # Token embedding
        embed_params = vocab_size * hidden_dim

        # Each transformer block
        # Attention: Q, K, V projections + output projection
        attn_params = (hidden_dim * hidden_dim) * 4  # Q, K, V, O

        # MLP: two linear layers (expand 4x then contract)
        mlp_params = (hidden_dim * 4 * hidden_dim) + (4 * hidden_dim * hidden_dim)
        mlp_params = 2 * (hidden_dim * 4 * hidden_dim)  # Simplified

        # Total per block
        block_params = attn_params + mlp_params

        # All blocks
        total_block_params = depth * block_params

        # LM head (unembedding)
        lm_head_params = vocab_size * hidden_dim

        # Total
        total_params = embed_params + total_block_params + lm_head_params

        return {
            'embedding': embed_params,
            'transformer_blocks': total_block_params,
            'lm_head': lm_head_params,
            'total': total_params,
            'blocks_breakdown': {
                'attention': attn_params,
                'mlp': mlp_params,
                'total_per_block': block_params
            }
        }

    def calculate_memory(self,
                        num_params: int,
                        batch_size: int,
                        seq_len: int,
                        hidden_dim: int,
                        depth: int) -> Dict:
        """
        Estimate GPU memory requirements.

        Args:
            num_params: Total number of parameters
            batch_size: Batch size
            seq_len: Sequence length
            hidden_dim: Hidden dimension
            depth: Number of layers

        Returns:
            Dictionary with memory breakdown in GB
        """
        # Model parameters (2 bytes per param for bf16)
        model_memory_gb = (num_params * 2) / (1024**3)

        # Optimizer states (AdamW: 2 states per param, 4 bytes each)
        optimizer_memory_gb = (num_params * 2 * 4) / (1024**3)

        # Gradients (2 bytes per param)
        gradient_memory_gb = (num_params * 2) / (1024**3)

        # Activations (rough estimate)
        # Each layer has activations of size: batch * seq_len * hidden_dim
        activation_elements = batch_size * seq_len * hidden_dim * depth
        activation_memory_gb = (activation_elements * 2) / (1024**3) * 10  # *10 for safety

        # Total
        total_memory_gb = model_memory_gb + optimizer_memory_gb + gradient_memory_gb + activation_memory_gb

        return {
            'model_params': model_memory_gb,
            'optimizer_states': optimizer_memory_gb,
            'gradients': gradient_memory_gb,
            'activations': activation_memory_gb,
            'total': total_memory_gb
        }

    def calculate_flops(self,
                       num_params: int,
                       seq_len: int,
                       depth: int,
                       hidden_dim: int,
                       n_heads: int) -> int:
        """
        Estimate FLOPs per token (from Chinchilla paper formula).

        Args:
            num_params: Total parameters
            seq_len: Sequence length
            depth: Number of layers
            hidden_dim: Hidden dimension
            n_heads: Number of heads

        Returns:
            FLOPs per token
        """
        # Approximate formula: 6N + 12LHdT
        # N = params, L = layers, H = heads, d = head_dim, T = seq_len
        head_dim = hidden_dim // n_heads
        flops_per_token = 6 * num_params + 12 * depth * n_heads * head_dim * seq_len

        return flops_per_token

    def estimate_training_time(self,
                              total_tokens: int,
                              flops_per_token: int,
                              gpu_tflops: float) -> Dict:
        """
        Estimate training time.

        Args:
            total_tokens: Total training tokens
            flops_per_token: FLOPs per token
            gpu_tflops: GPU throughput in TFLOPS

        Returns:
            Time estimates
        """
        # Total FLOPs
        total_flops = total_tokens * flops_per_token

        # Convert TFLOPS to FLOPS/sec
        flops_per_second = gpu_tflops * 1e12

        # Account for efficiency (typically 30-50% of peak)
        efficiency = 0.4
        effective_flops_per_second = flops_per_second * efficiency

        # Time in seconds
        time_seconds = total_flops / effective_flops_per_second

        # Convert to hours
        time_hours = time_seconds / 3600

        return {
            'seconds': time_seconds,
            'hours': time_hours,
            'days': time_hours / 24
        }

    def suggest_batch_size(self, gpu_memory_gb: float, total_memory_required_gb: float, device_batch_size: int) -> Dict:
        """
        Suggest optimal batch size for given GPU.

        Args:
            gpu_memory_gb: Available GPU memory
            total_memory_required_gb: Memory required for current config
            device_batch_size: Current batch size

        Returns:
            Suggested configuration
        """
        # Memory is roughly linear with batch size
        memory_per_batch = total_memory_required_gb / device_batch_size

        # Leave 20% headroom
        usable_memory = gpu_memory_gb * 0.8

        # Calculate max batch size
        max_batch_size = int(usable_memory / memory_per_batch)

        # Round down to power of 2
        suggested_batch_size = 2 ** int(math.log2(max_batch_size))

        return {
            'current_batch_size': device_batch_size,
            'max_batch_size': max_batch_size,
            'suggested_batch_size': suggested_batch_size,
            'memory_per_batch_gb': memory_per_batch
        }

    def print_report(self,
                    depth: int,
                    hidden_dim: int = None,
                    vocab_size: int = 65536,
                    batch_size: int = 32,
                    seq_len: int = 2048,
                    total_tokens: int = 54_000_000_000,
                    gpu_type: str = 'H100'):
        """
        Print comprehensive report.

        Args:
            depth: Model depth
            hidden_dim: Hidden dimension (calculated if None)
            vocab_size: Vocabulary size
            batch_size: Batch size
            seq_len: Sequence length
            total_tokens: Total training tokens
            gpu_type: GPU type for cost estimation
        """
        # Calculate hidden dim if not provided (aspect ratio 64)
        if hidden_dim is None:
            hidden_dim = depth * 64

        n_heads = max(1, (hidden_dim + 127) // 128)

        # Calculate parameters
        params = self.calculate_parameters(depth, hidden_dim, vocab_size, n_heads)

        # Calculate memory
        memory = self.calculate_memory(
            params['total'],
            batch_size,
            seq_len,
            hidden_dim,
            depth
        )

        # Calculate FLOPs
        flops = self.calculate_flops(
            params['total'],
            seq_len,
            depth,
            hidden_dim,
            n_heads
        )

        # Get GPU specs
        gpu = self.GPU_SPECS.get(gpu_type, self.GPU_SPECS['H100'])

        # Estimate training time
        time_est = self.estimate_training_time(
            total_tokens,
            flops,
            gpu['tflops']
        )

        # Suggest batch size
        batch_suggestion = self.suggest_batch_size(
            gpu['memory_gb'],
            memory['total'],
            batch_size
        )

        # Calculate cost
        cost = time_est['hours'] * gpu['cost_per_hour']

        # Print report
        print("\n" + "="*80)
        print(f"MODEL CONFIGURATION REPORT")
        print("="*80 + "\n")

        print("Model Architecture:")
        print(f"  Depth (layers):     {depth}")
        print(f"  Hidden dimension:   {hidden_dim}")
        print(f"  Number of heads:    {n_heads}")
        print(f"  Vocabulary size:    {vocab_size:,}")
        print(f"  Sequence length:    {seq_len}")

        print(f"\nParameters:")
        print(f"  Embedding:          {params['embedding']/1e6:.1f}M")
        print(f"  Transformer blocks: {params['transformer_blocks']/1e6:.1f}M")
        print(f"  LM head:            {params['lm_head']/1e6:.1f}M")
        print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  Total:              {params['total']/1e6:.1f}M ({params['total']/1e9:.2f}B)")

        print(f"\nMemory Requirements (batch_size={batch_size}):")
        print(f"  Model parameters:   {memory['model_params']:.2f} GB")
        print(f"  Optimizer states:   {memory['optimizer_states']:.2f} GB")
        print(f"  Gradients:          {memory['gradients']:.2f} GB")
        print(f"  Activations:        {memory['activations']:.2f} GB")
        print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  Total:              {memory['total']:.2f} GB")

        print(f"\nGPU: {gpu_type} ({gpu['memory_gb']} GB)")
        fits = "âœ“ FITS" if memory['total'] <= gpu['memory_gb'] else "âœ— TOO LARGE"
        print(f"  Status:             {fits}")

        print(f"\nBatch Size Suggestions:")
        print(f"  Current:            {batch_suggestion['current_batch_size']}")
        print(f"  Maximum:            {batch_suggestion['max_batch_size']}")
        print(f"  Suggested:          {batch_suggestion['suggested_batch_size']}")

        print(f"\nTraining Estimates:")
        print(f"  Total tokens:       {total_tokens/1e9:.1f}B")
        print(f"  FLOPs per token:    {flops:.2e}")
        print(f"  Total FLOPs:        {total_tokens * flops:.2e}")
        print(f"  Training time:      {time_est['hours']:.1f} hours ({time_est['days']:.2f} days)")
        print(f"  Estimated cost:     ${cost:.2f}")

        print("\n" + "="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Model Size and Cost Calculator")
    parser.add_argument("--depth", type=int, required=True, help="Model depth (number of layers)")
    parser.add_argument("--hidden-dim", type=int, help="Hidden dimension (default: depth * 64)")
    parser.add_argument("--vocab-size", type=int, default=65536, help="Vocabulary size")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size")
    parser.add_argument("--seq-len", type=int, default=2048, help="Sequence length")
    parser.add_argument("--total-tokens", type=float, default=54, help="Total training tokens (in billions)")
    parser.add_argument("--gpu", choices=list(ModelCalculator.GPU_SPECS.keys()),
                       default='H100', help="GPU type for cost estimation")

    args = parser.parse_args()

    calculator = ModelCalculator()
    calculator.print_report(
        depth=args.depth,
        hidden_dim=args.hidden_dim,
        vocab_size=args.vocab_size,
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        total_tokens=int(args.total_tokens * 1e9),
        gpu_type=args.gpu
    )


if __name__ == "__main__":
    main()
```

#### Usage Examples

```bash
# Calculate for d20 model
python tools/model_calculator.py --depth 20

# d26 model with custom settings
python tools/model_calculator.py --depth 26 --batch-size 16 --gpu A100

# d32 model
python tools/model_calculator.py --depth 32 --total-tokens 100
```

#### Expected Output

```
================================================================================
MODEL CONFIGURATION REPORT
================================================================================

Model Architecture:
  Depth (layers):     20
  Hidden dimension:   1280
  Number of heads:    10
  Vocabulary size:    65,536
  Sequence length:    2048

Parameters:
  Embedding:          83.9M
  Transformer blocks: 393.2M
  LM head:            83.9M
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:              561.2M (0.56B)

Memory Requirements (batch_size=32):
  Model parameters:   1.05 GB
  Optimizer states:   4.19 GB
  Gradients:          1.05 GB
  Activations:        26.21 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:              32.49 GB

GPU: H100 (80 GB)
  Status:             âœ“ FITS

Batch Size Suggestions:
  Current:            32
  Maximum:            79
  Suggested:          64

Training Estimates:
  Total tokens:       54.0B
  FLOPs per token:    3.67e+12
  Total FLOPs:        1.98e+23
  Training time:      4.5 hours (0.19 days)
  Estimated cost:     $15.75

================================================================================
```

### Learning Outcomes

- âœ… Understanding parameter counting
- âœ… Memory estimation techniques
- âœ… FLOP calculations
- âœ… GPU specifications and constraints
- âœ… Cost-benefit analysis

---

Due to length constraints, I'll create a separate continuation file for the remaining features (6-10). Would you like me to continue?