#!/usr/bin/env python3
"""
mac_inference.py - Run nanochat inference on Mac (with MPS GPU support)

Usage:
    python mac_inference.py <model_dir> [prompt]
    
Example:
    python mac_inference.py nanochat-d20-hf "The future of AI is"
"""

import torch
import json
import sys
from pathlib import Path

def check_mac_gpu():
    """Check Mac GPU (MPS) availability"""
    if torch.backends.mps.is_available():
        print("âœ… Mac GPU (MPS) is available")
        return torch.device('mps')
    else:
        print("âš ï¸  Mac GPU (MPS) not available, using CPU")
        return torch.device('cpu')

class NanoChatMacInference:
    """Mac-optimized nanochat inference with MPS support"""
    
    def __init__(self, model_dir):
        self.model_dir = Path(model_dir)
        self.device = check_mac_gpu()
        
        # Load configuration
        config_path = self.model_dir / "config.json"
        if not config_path.exists():
            raise FileNotFoundError(f"Config not found: {config_path}")
            
        with open(config_path) as f:
            self.config = json.load(f)
        
        # Load model weights
        model_path = self.model_dir / "pytorch_model.bin"
        if not model_path.exists():
            raise FileNotFoundError(f"Model weights not found: {model_path}")
            
        print(f"ğŸ“¦ Loading model weights...")
        self.state_dict = torch.load(model_path, map_location=self.device)
        
        # Load training metadata
        metadata_path = self.model_dir / "training_metadata.json"
        if metadata_path.exists():
            with open(metadata_path) as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {}
        
        print(f"âœ… Model loaded on {self.device}")
        self._print_model_info()
    
    def _print_model_info(self):
        """Print model information"""
        print(f"\nğŸ“‹ Model Information:")
        print(f"   ğŸ“ Directory: {self.model_dir}")
        print(f"   ğŸ”¥ Device: {self.device}")
        print(f"   ğŸ“ Layers: {self.config['num_hidden_layers']}")
        print(f"   ğŸ§  Hidden size: {self.config['hidden_size']}")
        print(f"   ğŸ‘ï¸  Attention heads: {self.config['num_attention_heads']}")
        print(f"   ğŸ“ Vocab size: {self.config['vocab_size']}")
        print(f"   ğŸ“ Max sequence: {self.config['max_position_embeddings']}")
        
        if self.metadata:
            step = self.metadata.get('step', 'unknown')
            loss = self.metadata.get('loss', 'unknown')
            print(f"   ğŸ“Š Training step: {step}")
            print(f"   ğŸ“‰ Final loss: {loss}")
        
        # Count parameters
        total_params = sum(p.numel() for p in self.state_dict.values())
        print(f"   ğŸ”¢ Parameters: {total_params:,}")
        
        # Check memory usage on MPS
        if self.device.type == 'mps':
            try:
                # Move a small tensor to check MPS memory
                test_tensor = torch.randn(10, 10).to(self.device)
                print(f"   âœ… MPS memory test passed")
                del test_tensor
            except Exception as e:
                print(f"   âš ï¸  MPS memory test failed: {e}")
    
    def generate_with_nanochat(self, prompt, max_tokens=100, temperature=0.8):
        """
        Generate text using nanochat model
        
        Note: This requires the actual nanochat model implementation.
        You need to:
        1. Import nanochat's GPT model class
        2. Create model instance with config
        3. Load state_dict
        4. Implement generation
        """
        
        print(f"\nğŸ¯ Generating text...")
        print(f"   ğŸ“ Prompt: '{prompt}'")
        print(f"   ğŸ² Max tokens: {max_tokens}")
        print(f"   ğŸŒ¡ï¸  Temperature: {temperature}")
        
        try:
            # This is where you'd implement actual generation
            # Example pseudocode:
            
            # from nanochat.model import GPT, GPTConfig
            # config = GPTConfig(**self.config['_nanochat_original_config'])
            # model = GPT(config)
            # model.load_state_dict(self.state_dict)
            # model = model.to(self.device)
            # model.eval()
            
            # with torch.no_grad():
            #     encoded = tokenize(prompt)
            #     tokens = torch.tensor(encoded).unsqueeze(0).to(self.device)
            #     generated = model.generate(tokens, max_new_tokens=max_tokens, temperature=temperature)
            #     result = detokenize(generated[0])
            #     return result
            
            print(f"   âš ï¸  Actual generation not implemented yet")
            print(f"   ğŸ’¡ Need to integrate with nanochat model classes")
            
            return f"[Placeholder: Generated {max_tokens} tokens from '{prompt}']"
            
        except Exception as e:
            print(f"   âŒ Generation error: {e}")
            return None
    
    def benchmark_speed(self):
        """Simple speed benchmark on Mac"""
        print(f"\nâ±ï¸  Running speed benchmark...")
        
        # Test tensor operations on the target device
        size = 1000
        iterations = 10
        
        import time
        
        start_time = time.time()
        for i in range(iterations):
            x = torch.randn(size, size).to(self.device)
            y = torch.randn(size, size).to(self.device)
            z = torch.mm(x, y)
            if self.device.type == 'mps':
                torch.mps.synchronize()  # Wait for MPS completion
        
        end_time = time.time()
        avg_time = (end_time - start_time) / iterations
        
        print(f"   ğŸš€ Average matrix multiplication ({size}x{size}): {avg_time:.4f}s")
        print(f"   ğŸ“Š Device performance: {'Good' if avg_time < 0.1 else 'Moderate' if avg_time < 0.5 else 'Slow'}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python mac_inference.py <model_dir> [prompt]")
        print("\nExample:")
        print("  python mac_inference.py nanochat-d20-hf 'The future of AI is'")
        sys.exit(1)
    
    model_dir = sys.argv[1]
    prompt = sys.argv[2] if len(sys.argv) > 2 else "Hello, I am"
    
    print("ğŸ nanochat Mac Inference")
    print("=" * 40)
    
    try:
        # Load model
        model = NanoChatMacInference(model_dir)
        
        # Run benchmark
        model.benchmark_speed()
        
        # Generate text (placeholder for now)
        result = model.generate_with_nanochat(prompt)
        if result:
            print(f"\nğŸ“ Generated:")
            print(f"   {result}")
        
        print(f"\nâœ… Mac inference demo complete!")
        print(f"ğŸ’¡ To enable actual generation, integrate with nanochat model classes")
        
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print(f"\nğŸ’¡ Make sure you:")
        print(f"   1. Ran download_weights.sh to get the checkpoints")
        print(f"   2. Ran convert_to_hf.py to create the HF format")
        print(f"   3. Specified the correct model directory")
        
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()