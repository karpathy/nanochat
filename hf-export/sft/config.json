{
  "architectures": [
    "NanoChatHFForCausalLM"
  ],
  "auto_map": {
    "AutoConfig": "configuration_nanochat.NanoChatHFConfig",
    "AutoModelForCausalLM": "modeling_nanochat.NanoChatHFForCausalLM",
    "AutoTokenizer": [
      "tokenization_nanochat.NanoChatTokenizer",
      null
    ]
  },
  "dtype": "float32",
  "model_type": "nanochat",
  "n_embd": 1280,
  "n_head": 10,
  "n_kv_head": 10,
  "n_layer": 20,
  "num_attention_heads": 10,
  "num_hidden_layers": 20,
  "num_key_value_heads": 10,
  "hidden_size": 1280,
  "max_position_embeddings": 2048,
  "sequence_len": 2048,
  "tie_word_embeddings": false,
  "tokenizer_class": "NanoChatTokenizer",
  "bos_token_id": 65527,
  "eos_token_id": 65531,
  "pad_token_id": 65527,
  "transformers_version": "4.57.3",
  "vocab_size": 65536
}
